{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"We are going to do the project for the Facial Expression Dataset. This is a popular dataset that contains around 7 different emotions with a lot of images. We have 7 classes and you can see the number of files in each class. In the class \"disgust\" we have less number of files. If we are having less number of files, we can use some class balancing using the Synthetic Minority Oversampling Technique (SMOTE) in order to uniform the class distribution. Other files seen to be well balanced. It's around like 4000. So that is fine for us. We also have a test image or the validation image with the corresponding labels. So we are going to use this dataset for this project. We are going to build a multi-class image classifiaction model with CNN. Now that we have explored the dataset, let's jump into the code!! ","metadata":{}},{"cell_type":"markdown","source":"## Import Modules","metadata":{}},{"cell_type":"markdown","source":"* <b>pandas</b> - used to perform data manipulation and analysis\n\n* <b>numpy</b> - used to perform a wide variety of mathematical operations on arrays\n\n* <b>matplotlib</b> - used for data visualization and graphical plotting\n\n* <b>seaborn</b> - built on top of matplotlib with similar functionalities\n\n* <b>os</b> - used to handle files using system commands\n\n* <b>tqdm</b> - progress bar decorator for iterators\n\n* <b>warnings</b> - to manipulate warnings details, filterwarnings('ignore') is to ignore the warnings thrown by the modules (gives clean results)\n\n* <b>load_img</b> - used for loading the image as numpy array\n\n* <b>tensorflow</b> – backend module for the use of Keras\n\n* <b>Dense</b> - single dimension linear layer\n\n* <b>Dropout</b> - used to add regularization to the data, avoiding over fitting & dropping out a fraction of the data\n\n* <b>Activation</b> - layer for the use of certain threshold\n\n* <b>Flatten</b> - convert a 2D array into a 1D array\n\n* <b>Conv2D</b> - convolutional layer in 2 dimension\n\n* <b>MaxPooling2D</b> - function to get the maximum pixel value to the next layer\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport random\nfrom tqdm.notebook import tqdm\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.utils import load_img\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D","metadata":{"execution":{"iopub.status.busy":"2023-08-21T21:50:01.516960Z","iopub.execute_input":"2023-08-21T21:50:01.517242Z","iopub.status.idle":"2023-08-21T21:50:12.126613Z","shell.execute_reply.started":"2023-08-21T21:50:01.517214Z","shell.execute_reply":"2023-08-21T21:50:12.125633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the Dataset","metadata":{}},{"cell_type":"code","source":"TRAIN_DIR = '../input/facial-expression-dataset/train/train/'\nTEST_DIR = '../input/facial-expression-dataset/test/test/'","metadata":{"execution":{"iopub.status.busy":"2023-08-21T21:50:12.128713Z","iopub.execute_input":"2023-08-21T21:50:12.129436Z","iopub.status.idle":"2023-08-21T21:50:12.133707Z","shell.execute_reply.started":"2023-08-21T21:50:12.129399Z","shell.execute_reply":"2023-08-21T21:50:12.132793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After this we have to define a function because we have to convert these folders into a complete image path with the corresponding labels. We are going to iterate over the directories. In the directories if we go further, we have the class labels and inside that we will be having the image file names.","metadata":{}},{"cell_type":"code","source":"def load_dataset(directory):\n    image_paths = []\n    labels = []\n    for label in os.listdir(directory):\n        for filename in os.listdir(directory+label):\n            image_path = os.path.join(directory, label, filename)\n            image_paths.append(image_path)\n            labels.append(label)\n        print(label, \"Completed\")\n    return image_paths, labels","metadata":{"execution":{"iopub.status.busy":"2023-08-21T21:50:12.135309Z","iopub.execute_input":"2023-08-21T21:50:12.135995Z","iopub.status.idle":"2023-08-21T21:50:12.150554Z","shell.execute_reply.started":"2023-08-21T21:50:12.135961Z","shell.execute_reply":"2023-08-21T21:50:12.149419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a function for converting the folder structure into a CSV. Now we will apply this function to get the training dataset. ","metadata":{}},{"cell_type":"code","source":"# Convert into dataframe\ntrain = pd.DataFrame()\ntrain['image'], train['label'] = load_dataset(TRAIN_DIR)\n# Shuffle the dataset\ntrain = train.sample(frac=1).reset_index(drop=True)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-21T21:50:12.153632Z","iopub.execute_input":"2023-08-21T21:50:12.154032Z","iopub.status.idle":"2023-08-21T21:50:30.743040Z","shell.execute_reply.started":"2023-08-21T21:50:12.153999Z","shell.execute_reply":"2023-08-21T21:50:30.741950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We get all the different samples for each label. Now we will apply this function to get the testing dataset. ","metadata":{}},{"cell_type":"code","source":"test = pd.DataFrame()\ntest['image'], test['label'] = load_dataset(TEST_DIR)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-21T21:50:30.744362Z","iopub.execute_input":"2023-08-21T21:50:30.745819Z","iopub.status.idle":"2023-08-21T21:50:44.511653Z","shell.execute_reply.started":"2023-08-21T21:50:30.745781Z","shell.execute_reply":"2023-08-21T21:50:44.510713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are going to use this for the validation purpose. Now the dataset has been loaded successfully. We will now go for exploratory data analysis. ","metadata":{}},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"sns.countplot(x = 'label', data = train)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T21:50:44.513067Z","iopub.execute_input":"2023-08-21T21:50:44.513940Z","iopub.status.idle":"2023-08-21T21:50:44.834289Z","shell.execute_reply.started":"2023-08-21T21:50:44.513903Z","shell.execute_reply":"2023-08-21T21:50:44.833338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the class distribution. The class \"happy\" has so many labels and otherwise for all the classes the labels are almost equally distributed. Only for the class \"disgust\" the labels are so less. That means the number of samples for our training data is so less. We may have to use some class balancing like SMOTE methods or Weighted Random Sampler to uniform that. After that we will just load a single image. ","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimg = Image.open(train['image'][0])\nplt.imshow(img, cmap='gray');","metadata":{"execution":{"iopub.status.busy":"2023-08-21T21:50:44.835782Z","iopub.execute_input":"2023-08-21T21:50:44.836377Z","iopub.status.idle":"2023-08-21T21:50:45.095480Z","shell.execute_reply.started":"2023-08-21T21:50:44.836339Z","shell.execute_reply":"2023-08-21T21:50:45.094524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The image is actually in grayscale only so that's why we have to apply this color map and you can see the image is for \"surprise\". This is the smaple image and you can see the width and height; it is around 48 X 48. So that is the resolution of the image. We are going to dispaly a 5 X 5 matrix of images for the top 25 samples. ","metadata":{}},{"cell_type":"code","source":"# To display grid of images\nplt.figure(figsize=(20,20))\nfiles = train.iloc[0:25]\n\nfor index, file, label in files.itertuples():\n    plt.subplot(5, 5, index+1)\n    img = load_img(file)\n    img = np.array(img)\n    plt.imshow(img)\n    plt.title(label)\n    plt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2023-08-21T21:50:45.096765Z","iopub.execute_input":"2023-08-21T21:50:45.097554Z","iopub.status.idle":"2023-08-21T21:50:47.385619Z","shell.execute_reply.started":"2023-08-21T21:50:45.097518Z","shell.execute_reply":"2023-08-21T21:50:47.384644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After that we have to extract the features of the image. We just have to convert that into a numpy array, load it and reshape it in the proper format. ","metadata":{}},{"cell_type":"markdown","source":"## Feature Extraction","metadata":{}},{"cell_type":"code","source":"def extract_features(images):\n    features = []\n    for image in tqdm(images):\n        img = load_img(image, grayscale=True)\n        img = np.array(img)\n        features.append(img)\n    features = np.array(features)\n    features = features.reshape(len(features), 48, 48, 1)\n    return features","metadata":{"execution":{"iopub.status.busy":"2023-08-21T21:50:47.386632Z","iopub.execute_input":"2023-08-21T21:50:47.386998Z","iopub.status.idle":"2023-08-21T21:50:47.394213Z","shell.execute_reply.started":"2023-08-21T21:50:47.386962Z","shell.execute_reply":"2023-08-21T21:50:47.393170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here tqdm creates a loading bar to help track the conversion process of the images. Pixel features of the images are extracted. Images are converted into arrays and reshaped to the proper format. We need to reshape the arrays inorder to feed it into the model. Now we extract the features from both train and test.","metadata":{}},{"cell_type":"code","source":"train_features = extract_features(train['image'])","metadata":{"execution":{"iopub.status.busy":"2023-08-21T21:50:47.399028Z","iopub.execute_input":"2023-08-21T21:50:47.399946Z","iopub.status.idle":"2023-08-21T21:57:13.303065Z","shell.execute_reply.started":"2023-08-21T21:50:47.399792Z","shell.execute_reply":"2023-08-21T21:57:13.302061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_features = extract_features(test['image'])","metadata":{"execution":{"iopub.status.busy":"2023-08-21T21:57:13.304656Z","iopub.execute_input":"2023-08-21T21:57:13.305033Z","iopub.status.idle":"2023-08-21T21:58:23.287889Z","shell.execute_reply.started":"2023-08-21T21:57:13.304999Z","shell.execute_reply":"2023-08-21T21:58:23.286941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After that we have to normalise the image. This converts the pixel value from 1 to 255 into a normalized range of 0 to 1. Normalization is useful for the neural network to easily capture the information. ","metadata":{}},{"cell_type":"code","source":"# Normalize the image\nx_train = train_features/255.0\nx_test = test_features/255.0","metadata":{"execution":{"iopub.status.busy":"2023-08-21T21:58:23.289305Z","iopub.execute_input":"2023-08-21T21:58:23.289884Z","iopub.status.idle":"2023-08-21T21:58:23.500093Z","shell.execute_reply.started":"2023-08-21T21:58:23.289848Z","shell.execute_reply":"2023-08-21T21:58:23.498917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have to do one more preprocessing step that is to convert label to integer for easier processing. Initially the labels for the images are in terms of string in the dataset. We have to convert it into integer. We will be using Label Encoding for that.  ","metadata":{}},{"cell_type":"code","source":"# Convert label to integer\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train['label'])\ny_train = le.transform(train['label'])\ny_test = le.transform(test['label'])","metadata":{"execution":{"iopub.status.busy":"2023-08-21T21:58:23.501445Z","iopub.execute_input":"2023-08-21T21:58:23.502413Z","iopub.status.idle":"2023-08-21T21:58:23.577838Z","shell.execute_reply.started":"2023-08-21T21:58:23.502374Z","shell.execute_reply":"2023-08-21T21:58:23.576915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Maybe we can convert this Label Encoding into One Hot Encoding because this neural network performs better when we are using One Hot Encoding. ","metadata":{}},{"cell_type":"code","source":"y_train = to_categorical(y_train, num_classes=7)\ny_test = to_categorical(y_test, num_classes=7)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T21:58:23.579289Z","iopub.execute_input":"2023-08-21T21:58:23.579662Z","iopub.status.idle":"2023-08-21T21:58:23.586384Z","shell.execute_reply.started":"2023-08-21T21:58:23.579629Z","shell.execute_reply":"2023-08-21T21:58:23.585275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train[0]","metadata":{"execution":{"iopub.status.busy":"2023-08-21T21:58:23.587977Z","iopub.execute_input":"2023-08-21T21:58:23.588383Z","iopub.status.idle":"2023-08-21T21:58:23.598968Z","shell.execute_reply.started":"2023-08-21T21:58:23.588347Z","shell.execute_reply":"2023-08-21T21:58:23.597953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# config\ninput_shape = (48, 48, 1)\noutput_class = 7","metadata":{"execution":{"iopub.status.busy":"2023-08-21T21:58:23.600325Z","iopub.execute_input":"2023-08-21T21:58:23.600856Z","iopub.status.idle":"2023-08-21T21:58:23.607810Z","shell.execute_reply.started":"2023-08-21T21:58:23.600823Z","shell.execute_reply":"2023-08-21T21:58:23.606798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* <b>input_shape = (48, 48, 1)</b> - Converts the input image into 48 x 48 resolution in grayscale\n\n* <b>output_class</b> = 7 - Total number of classes\n\n","metadata":{}},{"cell_type":"markdown","source":"## Model Creation","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\n# 1st Convolutional Layer (create 1 full layer and reuse the same information)\nmodel.add(Conv2D(128, kernel_size=(3,3), activation='relu', input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.4))  # Add a dropout layer because we don't want to overfit the dataset\n\n# 2nd Convolutional Layer\nmodel.add(Conv2D(256, kernel_size=(3,3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.4))\n\n# 3rd Convolutional Layer\nmodel.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.4))\n\n# 4th Convolutional Layer\nmodel.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.4))\n\nmodel.add(Flatten())\n\n# Fully connected layers\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.3))\n\n# Output layer\nmodel.add(Dense(output_class, activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')","metadata":{"execution":{"iopub.status.busy":"2023-08-21T21:58:23.609357Z","iopub.execute_input":"2023-08-21T21:58:23.609707Z","iopub.status.idle":"2023-08-21T21:58:26.684807Z","shell.execute_reply.started":"2023-08-21T21:58:23.609676Z","shell.execute_reply":"2023-08-21T21:58:26.683779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can add Batch normalisation and check whether it it is improving the accuracy or not. Usually we can ignore the dropout layer if we have implemented image augmentation. We are not doing any kind of augmentation here because the images are small so we are just using as it is. \n\n* <b>optimizer=’adam’</b> - automatically adjust the learning rate for the model over the no. of epochs\n\n* <b>activation='softmax'</b> - used for multi-classification output\n\n* <b>loss=’categorical_crossentropy’</b> - loss function for category outputs","metadata":{}},{"cell_type":"code","source":"# train the model\nhistory = model.fit(x=x_train, y=y_train, batch_size=128, epochs=100, validation_data=(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-08-21T21:58:26.686305Z","iopub.execute_input":"2023-08-21T21:58:26.686663Z","iopub.status.idle":"2023-08-21T22:10:29.806603Z","shell.execute_reply.started":"2023-08-21T21:58:26.686628Z","shell.execute_reply":"2023-08-21T22:10:29.805616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot the Results","metadata":{}},{"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'b', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\nplt.title('Accuracy Graph')\nplt.legend()\nplt.figure()\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(acc))\n\nplt.plot(epochs, loss, 'b', label='Training Loss')\nplt.plot(epochs, val_loss, 'r', label='Validation Loss')\nplt.title('Loss Graph')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-21T22:10:29.808333Z","iopub.execute_input":"2023-08-21T22:10:29.808694Z","iopub.status.idle":"2023-08-21T22:10:30.343347Z","shell.execute_reply.started":"2023-08-21T22:10:29.808659Z","shell.execute_reply":"2023-08-21T22:10:30.342431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test with Image Data","metadata":{}},{"cell_type":"markdown","source":"Now we will display a few random images with its label and predict the output.","metadata":{}},{"cell_type":"code","source":"image_index = random.randint(0, len(test))\nprint(\"Original Output:\", test['label'][image_index])\npred = model.predict(x_test[image_index].reshape(1, 48, 48, 1))\nprediction_label = le.inverse_transform([pred.argmax()])[0]\nprint(\"Predicted Output:\", prediction_label)\nplt.imshow(x_test[image_index].reshape(48, 48), cmap='gray');","metadata":{"execution":{"iopub.status.busy":"2023-08-21T22:10:30.344795Z","iopub.execute_input":"2023-08-21T22:10:30.345811Z","iopub.status.idle":"2023-08-21T22:10:30.956541Z","shell.execute_reply.started":"2023-08-21T22:10:30.345768Z","shell.execute_reply":"2023-08-21T22:10:30.955632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_index = random.randint(0, len(test))\nprint(\"Original Output:\", test['label'][image_index])\npred = model.predict(x_test[image_index].reshape(1, 48, 48, 1))\nprediction_label = le.inverse_transform([pred.argmax()])[0]\nprint(\"Predicted Output:\", prediction_label)\nplt.imshow(x_test[image_index].reshape(48, 48), cmap='gray');","metadata":{"execution":{"iopub.status.busy":"2023-08-21T22:10:30.960738Z","iopub.execute_input":"2023-08-21T22:10:30.962898Z","iopub.status.idle":"2023-08-21T22:10:31.370383Z","shell.execute_reply.started":"2023-08-21T22:10:30.962862Z","shell.execute_reply":"2023-08-21T22:10:31.369448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_index = random.randint(0, len(test))\nprint(\"Original Output:\", test['label'][image_index])\npred = model.predict(x_test[image_index].reshape(1, 48, 48, 1))\nprediction_label = le.inverse_transform([pred.argmax()])[0]\nprint(\"Predicted Output:\", prediction_label)\nplt.imshow(x_test[image_index].reshape(48, 48), cmap='gray');","metadata":{"execution":{"iopub.status.busy":"2023-08-21T22:11:16.783839Z","iopub.execute_input":"2023-08-21T22:11:16.784211Z","iopub.status.idle":"2023-08-21T22:11:17.085602Z","shell.execute_reply.started":"2023-08-21T22:11:16.784181Z","shell.execute_reply":"2023-08-21T22:11:17.084665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we see random display of images with original output and predicted output. In some cases the output may differ due to facial expression similarities like fear and surprise. Overall the model predicted very good results. ","metadata":{}},{"cell_type":"markdown","source":"## Final Thoughts","metadata":{}},{"cell_type":"markdown","source":"* Training the model by increasing the no. of epochs can give better and more accurate results.\n\n* Processing large amount of data can take a lot of time and system resources.\n\n* Basic deep learning model trained in a neural network, adding new layers varies the results.\n\n","metadata":{}}]}